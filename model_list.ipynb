{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0b4ee643",
   "metadata": {},
   "source": [
    "# Model List\n",
    "\n",
    "[![Binder](https://mybinder.org/badge_logo.svg)](https://mybinder.org/v2/gh/https://github.com/9juhnke/llm-api-gwdg-saia/main?filepath=model_list.ipynb)\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/9juhnke/llm-api-gwdg-saia/blob/main/model_list.ipynb)\n",
    "\n",
    "This notebook shows how you can display a list of all models currently available via the SAIA API (see [README.md](./README.md)) - including supported input and output modalities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45acf138",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To install the required packages remove the comment character before the next line\n",
    "# !pip install openai python-dotenv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58156f71",
   "metadata": {},
   "source": [
    "For more information on the respective models see the [model list](https://docs.hpc.gwdg.de/services/chat-ai/models/index.html). \n",
    "\n",
    "A complete up-to-date list of available models can be retrieved via the following command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d58979e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "meta-llama-3.1-8b-instruct, ['text']->['text']\n",
      "gemma-3-27b-it, ['text', 'image']->['text']\n",
      "qwen3-32b, ['text']->['text', 'thought']\n",
      "llama-3.3-70b-instruct, ['text']->['text']\n",
      "qwen3-235b-a22b, ['text']->['text', 'thought']\n",
      "qwen2.5-vl-72b-instruct, ['text', 'image', 'video']->['text']\n",
      "qwq-32b, ['text']->['text', 'thought']\n",
      "deepseek-r1, ['text']->['text', 'thought']\n",
      "deepseek-r1-distill-llama-70b, ['text']->['text', 'thought']\n",
      "mistral-large-instruct, ['text']->['text']\n",
      "qwen2.5-coder-32b-instruct, ['text']->['text']\n",
      "internvl2.5-8b, ['text', 'image']->['text']\n",
      "codestral-22b, ['text']->['text']\n",
      "llama-3.1-sauerkrautlm-70b-instruct, ['text', 'arcana']->['text']\n",
      "meta-llama-3.1-8b-rag, ['text', 'arcana']->['text']\n"
     ]
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "# API configuration\n",
    "api_key = \"<Your API_KEY>\" # Insert your API Key\n",
    "base_url = \"https://chat-ai.academiccloud.de/v1\"\n",
    "\n",
    "# Start OpenAI client\n",
    "client = OpenAI(\n",
    "    api_key=api_key, \n",
    "    base_url=base_url\n",
    "    )\n",
    "\n",
    "# Get response\n",
    "models = client.models.list()\n",
    "\n",
    "# Print full response as JSON or extract the relevant response text from the JSON object\n",
    "for model in models.model_dump()[\"data\"]:\n",
    "    print(f\"{model['id']}, {model['input']}->{model['output']}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "64-869 Python3 (.venv)",
   "language": "python",
   "name": ".venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
